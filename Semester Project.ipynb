{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b43ce78b-9e46-43fc-90a5-471be5689212",
   "metadata": {},
   "source": [
    "# Machine Learning 2 - Semester Project\n",
    "# Garbage Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979aaa26-587c-4024-b047-40593d3cd55d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Revised Order:\n",
    "\n",
    "1. **Data Collection:**\n",
    "   - Collect a dataset of images representing different types of garbage items (plastic, paper, glass, etc.).\n",
    "   - Found multiple data sets on kaggle\n",
    "    \n",
    "2. **Image Classification:**\n",
    "   - Use convolutional neural networks (CNNs) to build an image classification model.\n",
    "   - Train the model to recognize and classify each type of garbage item.\n",
    "   - Evaluate the model's performance using appropriate metrics.\n",
    "\n",
    "3. **Pre-trained Models Comparison:**\n",
    "   - Choose popular pre-trained models for image classification (e.g., ResNet, VGG, MobileNet).\n",
    "   - Fine-tune these models on your garbage classification dataset.\n",
    "   - Evaluate the performance of the fine-tuned models.\n",
    "   - Compare the performance metrics with your custom-trained model.\n",
    "\n",
    "4. **Clustering and Dimension Reduction:**\n",
    "   - Apply clustering algorithms (e.g., K-Means) to group similar garbage items together.\n",
    "        - (Hope to find clusters like: recycleable/ bio/ others\n",
    "   - Use dimension reduction techniques (e.g., PCA) to visualize and analyze the features of the garbage items.\n",
    "\n",
    "5. **Integrating Recommendations:**\n",
    "   - Implement a recommendation system that suggests the appropriate recycling bin for a given item.\n",
    "        - maybe simple tree??\n",
    "   - Use knowledge from the clustering and dimension reduction to enhance recommendation accuracy.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea497cd-08a0-4b70-b667-e1520fabb9b0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Training Metrics Explanation:</b>\n",
    "\n",
    "- **loss:**\n",
    "  - The training loss at the end of the epoch. It represents the average value of the loss function across all training samples.\n",
    "\n",
    "- **accuracy:**\n",
    "  - The training accuracy at the end of the epoch. It represents the proportion of correctly classified training samples.\n",
    "\n",
    "- **val_loss:**\n",
    "  - The validation loss at the end of the epoch. It represents the average value of the loss function across all validation samples.\n",
    "  \n",
    "  Categorical Crossentropy Loss Formula:\n",
    "  \n",
    "  \n",
    "  Categorical Crossentropy Loss =$ - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{i,j} \\cdot \\log(\\hat{y}_{i,j})$\n",
    "\n",
    "  Where:\n",
    "    - $N$ is the number of samples in the batch.\n",
    "    - $C$ is the number of classes.\n",
    "    - $y_{i,j}$ is 1 if the true class of sample \\(i\\) is \\(j\\), 0 otherwise.\n",
    "    - $(\\hat{y}_{i,j})$ is the predicted probability that sample \\(i\\) belongs to class \\(j\\).\n",
    "\n",
    "- **val_accuracy:**\n",
    "  - The validation accuracy at the end of the epoch. It represents the proportion of correctly classified validation samples.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a8e570e-9e00-4bfa-9b99-4980ff6c6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#!pip install ann_visualizer\n",
    "# Libraries for Images\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "# Libraries for Evaluations and Train/Test Split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Model Libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "#!pip install keras-tuner\n",
    "#import keras_turner\n",
    "from keras_tuner.tuners import BayesianOptimization\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "#VGG16\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#ResNET\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#Statistical Test\n",
    "from scipy.stats import ttest_rel\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import ttest_rel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#Model Visualization\n",
    "from ann_visualizer.visualize import ann_viz\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "#Others\n",
    "from tqdm import tqdm  # Optional: tqdm for progress bar\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import clone_model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "\n",
    "#Own functions\n",
    "import sys\n",
    "from Own_Functions.k_fold_utils import k_fold_cross_validation\n",
    "from Own_Functions.compare_models import compare_models\n",
    "from Own_Functions.compare_model_performance import compare_model_performance\n",
    "from Own_Functions.check_compile import check_compile\n",
    "from Own_Functions.count_images_in_subfolders import count_images_in_subfolders\n",
    "from Own_Functions.evaluate_model import evaluate_model\n",
    "from Own_Functions.save_results_to_json import save_results_to_json\n",
    "from Own_Functions.save_history_to_json import save_history_to_json\n",
    "from Own_Functions.subgenerator_utils import create_subgenerator \n",
    "from Own_Functions.plot_metric_across_folds import plot_metric_across_folds \n",
    "from Own_Functions.plot_comparison_across_folds import plot_comparison_across_folds "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bb83c5",
   "metadata": {},
   "source": [
    "## Download Data for Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbc289f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classes</th>\n",
       "      <th>Number_Images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paper</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>green-glass</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clothes</td>\n",
       "      <td>5325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metal</td>\n",
       "      <td>769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cardboard</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>trash</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>biological</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>white-glass</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>battery</td>\n",
       "      <td>945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>brown-glass</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>plastic</td>\n",
       "      <td>865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>shoes</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Classes  Number_Images\n",
       "0         paper           1050\n",
       "1   green-glass            629\n",
       "2       clothes           5325\n",
       "3         metal            769\n",
       "4     cardboard            891\n",
       "5         trash            697\n",
       "6    biological            985\n",
       "7   white-glass            775\n",
       "8       battery            945\n",
       "9   brown-glass            607\n",
       "10      plastic            865\n",
       "11        shoes           1977"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage:\n",
    "root_folder_path = '/Users/linuszarse/Documents/UNI/Master-Uni Potsdam/3. Semester/Machine Learning 2/Klausurprojekt/garbage_classification/'\n",
    "image_overview = count_images_in_subfolders(root_folder_path)\n",
    "\n",
    "image_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12dd15ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15515"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_overview['Number_Images'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a615ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(image_overview, x=\"Classes\", y=\"Number_Images\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4647879",
   "metadata": {},
   "source": [
    "### Use ImageData Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbe7fab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12415 images belonging to 12 classes.\n",
      "Found 3100 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "root_folder_path = '/Users/linuszarse/Documents/UNI/Master-Uni Potsdam/3. Semester/Machine Learning 2/Klausurprojekt/garbage_classification'\n",
    "\n",
    "# Define the ImageDataGenerator for data augmentation and preprocessing\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,       # Rescale pixel values to be between 0 and 1\n",
    "    shear_range=0.2,      # Shear transformation\n",
    "    zoom_range=0.2,       # Random zoom\n",
    "    horizontal_flip=True, # Random horizontal flip\n",
    "    validation_split=0.2,  # Split data into training and validation sets\n",
    "    vertical_flip=True,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    "\n",
    "# Define the target image size\n",
    "target_size = (256,256)  # Adjust according to your model's input size\n",
    "\n",
    "# Create the training data generator\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    root_folder_path,\n",
    "    target_size=target_size,\n",
    "    batch_size=32,         # Adjust based on your system's memory\n",
    "    class_mode='categorical',  # Assumes a classification task\n",
    "    subset='training'      # Specify 'training' for training data\n",
    ")\n",
    "\n",
    "# Create the validation data generator\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    root_folder_path,\n",
    "    target_size=target_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'    # Specify 'validation' for validation data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d369274f-6af8-4f08-b4f1-1fb1a16fefd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9309 validated image filenames belonging to 12 classes.\n",
      "Found 3103 validated image filenames belonging to 12 classes.\n",
      "Found 3103 validated image filenames belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Pfad zum Hauptverzeichnis der Daten\n",
    "root_folder_path = '/Users/linuszarse/Documents/UNI/Master-Uni Potsdam/3. Semester/Machine Learning 2/Klausurprojekt/garbage_classification'\n",
    "\n",
    "# Liste zum Speichern der Dateipfade und Labels\n",
    "file_paths = []\n",
    "labels = []\n",
    "\n",
    "# Angenommen, jedes Unterverzeichnis im Root-Verzeichnis entspricht einer Klasse\n",
    "for class_name in os.listdir(root_folder_path):\n",
    "    class_dir = os.path.join(root_folder_path, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            file_paths.append(os.path.join(class_name, file_name))  # Speichere relativen Pfad\n",
    "            labels.append(class_name)\n",
    "\n",
    "# Konvertiere Listen in Arrays für die Verarbeitung\n",
    "file_paths = np.array(file_paths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Aufteilung der Daten in Training+Validierung und Test, stratifizierte Aufteilung\n",
    "file_paths_train_val, file_paths_test, labels_train_val, labels_test = train_test_split(\n",
    "    file_paths, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Aufteilung der Trainings- und Validierungsdaten, stratifizierte Aufteilung\n",
    "file_paths_train, file_paths_val, labels_train, labels_val = train_test_split(\n",
    "    file_paths_train_val, labels_train_val, test_size=0.25, random_state=42, stratify=labels_train_val)\n",
    "\n",
    "\n",
    "# Initialisiere ImageDataGenerator-Instanzen für Training und Validierung/Test ohne Augmentierung\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Da wir die Dateipfade und Labels direkt haben, könnten wir hier flow_from_dataframe oder eine ähnliche Methode benötigen\n",
    "# Dies erfordert, dass du ein DataFrame aus den Pfaden und Labels erstellst\n",
    "\n",
    "# Erstelle DataFrames\n",
    "df_train = pd.DataFrame({'filepath': file_paths_train, 'label': labels_train})\n",
    "df_val = pd.DataFrame({'filepath': file_paths_val, 'label': labels_val})\n",
    "df_test = pd.DataFrame({'filepath': file_paths_test, 'label': labels_test})\n",
    "\n",
    "# Initialisiere ImageDataGenerator für Training mit Augmentierung\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Für Validierung und Test ohne Augmentierung, nur Reskalierung\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Erstelle den Trainingsdatengenerator\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=df_train,\n",
    "    directory=root_folder_path,\n",
    "    x_col='filepath',\n",
    "    y_col='label',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Erstelle den Validierungsdatengenerator\n",
    "validation_generator = val_test_datagen.flow_from_dataframe(\n",
    "    dataframe=df_val,\n",
    "    directory=root_folder_path,\n",
    "    x_col='filepath',\n",
    "    y_col='label',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Erstelle den Testdatengenerator\n",
    "test_generator = val_test_datagen.flow_from_dataframe(\n",
    "    dataframe=df_test,\n",
    "    directory=root_folder_path,\n",
    "    x_col='filepath',\n",
    "    y_col='label',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Wichtig für die Bewertung, um die Reihenfolge beizubehalten\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27253b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Names: ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n"
     ]
    }
   ],
   "source": [
    "class_names = list(train_generator.class_indices.keys())\n",
    "print(\"Class Names:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83493a20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get a batch from the training generator\n",
    "#batch_images, batch_labels = next(train_generator)\n",
    "\n",
    "# Visualize the first few images in the batch\n",
    "#num_images_to_visualize = 12\n",
    "#for i in range(num_images_to_visualize):\n",
    "    # Get the image and label\n",
    "#    image = batch_images[i]\n",
    "#    label_index = np.argmax(batch_labels[i])\n",
    "    \n",
    "    # Plot the image\n",
    "#    plt.imshow(image)\n",
    "#    plt.title(f\"Label: {label_index}\")\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc6d9595",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get a batch from the training generator\n",
    "#batch_images, batch_labels = next(validation_generator)\n",
    "\n",
    "# Visualize the first few images in the batch\n",
    "#num_images_to_visualize = 5\n",
    "#for i in range(num_images_to_visualize):\n",
    "#    # Get the image and label\n",
    "#    image = batch_images[i]\n",
    "#    label_index = np.argmax(batch_labels[i])\n",
    "#    \n",
    "#    # Plot the image\n",
    "#    plt.imshow(image)\n",
    "#    plt.title(f\"Label: {label_index}\")\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a4a0ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first few images in the batch with class names\n",
    "#for i in range(num_images_to_visualize):\n",
    "#    # Get the image and label\n",
    "#    image = batch_images[i]\n",
    "#    label_index = np.argmax(batch_labels[i])\n",
    "#    class_name = class_names[label_index]\n",
    "#    \n",
    "#    # Plot the image\n",
    "#    plt.imshow(image)\n",
    "#    plt.title(f\"Class Name: {class_name}\")\n",
    "#    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2224937b",
   "metadata": {},
   "source": [
    "***\n",
    "## Definition of Test Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce16eb4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9903d22c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897936a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fee4886a",
   "metadata": {},
   "source": [
    "*** \n",
    "## Definition K-Fold Cross Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd342b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00fcc6f3-8d1a-4d43-b4c4-03997264e0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f641ca9-48bf-4ac5-8a32-d30e1bba84d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "721f6f30-0e70-41fd-9483-799200d10c30",
   "metadata": {},
   "source": [
    "***\n",
    "### Visualisierungen Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "703c1ff5-7fb6-4454-bb68-17efac9df7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1edd49-f184-4dfe-8c0d-ae0278a3b27c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b7c6400",
   "metadata": {},
   "source": [
    "***\n",
    "## Implementing CNN - own build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88aa591b-3c2f-4cd3-a992-a68c9b1f5404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'battery': 1.3681657848324515,\n",
       " 'biological': 1.3126057529610828,\n",
       " 'brown-glass': 2.131181318681319,\n",
       " 'cardboard': 1.45,\n",
       " 'clothes': 0.24280125195618155,\n",
       " 'green-glass': 2.0576923076923075,\n",
       " 'metal': 1.6827548806941433,\n",
       " 'paper': 1.2313492063492064,\n",
       " 'plastic': 1.4947013487475915,\n",
       " 'shoes': 0.6540893760539629,\n",
       " 'trash': 1.8514319809069213,\n",
       " 'white-glass': 1.668279569892473}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Klassen und ihre Frequenz\n",
    "y_integers = np.array(train_generator.classes)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_integers), y=y_integers)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Umwandlung von Klassenindizes in ein Dictionary mit Klassennamen als Schlüssel\n",
    "class_indices = train_generator.class_indices  # Sollte ein Dictionary sein: {'class1': 0, 'class2': 1, ...}\n",
    "class_weights_dict\n",
    "# Erstellen eines neuen Dictionaries, das Klassennamen den Gewichten zuordnet\n",
    "class_weights_by_name = {class_name: class_weights_dict[class_index] for class_name, class_index in class_indices.items()}\n",
    "class_weights_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdab687-63f2-42a3-a486-009d4a75e61c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab44e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "num_classes = len(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a649c56",
   "metadata": {},
   "source": [
    "## CNN2 - max pooling, relu, batch normalization, dropout, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93912920-63ec-421b-bf12-e963cb77f148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell erfolgreich geladen.\n"
     ]
    }
   ],
   "source": [
    "# Set the path for the model\n",
    "model_path = '/Users/linuszarse/Documents/UNI/Master-Uni Potsdam/3. Semester/Machine Learning 2/Klausurprojekt/Best_Models/'\n",
    "model_filename = 'best_model_cnn2.keras'\n",
    "checkpoint_path = os.path.join(model_path, model_filename)\n",
    "\n",
    "# Check if the model exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    # Load the model\n",
    "    model_cnn2 = load_model(checkpoint_path)\n",
    "    print(\"Modell erfolgreich geladen.\")\n",
    "else:\n",
    "    # Define the model\n",
    "    model_cnn2 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model_cnn2.compile(optimizer=tf.keras.optimizers.legacy.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Display the model summary\n",
    "    model_cnn2.summary()\n",
    "\n",
    "    #checkpoint = ModelCheckpoint(\"best_model.keras\", monitor='val_loss', save_best_only=True)\n",
    "    #checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True)\n",
    "    \n",
    "    # Define a callback to save the best model during training\n",
    "    #checkpoint = ModelCheckpoint(os.path.join(model_path, 'best_model_cnn2.keras'), monitor='val_loss', save_best_only=True)\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "    # Train the model\n",
    "    history_cnn2 = model_cnn2.fit(\n",
    "        train_generator,\n",
    "        epochs=2,  # Adjust the number of epochs based on your needs\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[checkpoint],\n",
    "        class_weight=class_weights_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7cd73-ffb7-4018-90fd-2385fae9f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = '/Users/linuszarse/Documents/UNI/Master-Uni Potsdam/3. Semester/Machine Learning 2/Klausurprojekt/Best_Models/' \n",
    "history_filename_cnn2_training_history = 'best_model_cnn2_training_history.json'\n",
    "\n",
    "# Speichern der Historie\n",
    "save_history_to_json(history_cnn2, history_filename_cnn2_training_history, history_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "570b584c-f1a9-4442-96e5-c3882f70f5fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'check_compile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m check_compile(model_cnn2)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'check_compile' is not defined"
     ]
    }
   ],
   "source": [
    "check_compile(model_cnn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8829a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_path, history_filename_cnn2_training_history), 'r') as file:\n",
    "    loaded_history = json.load(file)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca93e7-389a-4565-b221-0429eae899db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c41b54dd-68cd-43eb-b8b0-97ea0a7494d5",
   "metadata": {},
   "source": [
    "***\n",
    "### Evalutation Protocoll: K-Fold Cross Validation for CNN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c254f19",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4654 validated image filenames belonging to 12 classes.\n",
      "Found 4655 validated image filenames belonging to 12 classes.\n",
      "145/145 [==============================] - 220s 2s/step - loss: 2.0923 - accuracy: 0.4414 - val_loss: 2.9120 - val_accuracy: 0.3136\n",
      "Found 4655 validated image filenames belonging to 12 classes.\n",
      "Found 4654 validated image filenames belonging to 12 classes.\n",
      "145/145 [==============================] - 234s 2s/step - loss: 2.1131 - accuracy: 0.4311 - val_loss: 2.8608 - val_accuracy: 0.2463\n"
     ]
    }
   ],
   "source": [
    "fold_performance_model_cnn2, avg_perf_model_cnn2, fold_histories_model_cnn2 = k_fold_cross_validation(model_cnn2, \n",
    "                                                                                                      train_generator, \n",
    "                                                                                                      n_splits=5, \n",
    "                                                                                                      epochs=20)\n",
    "\n",
    "#fange mit 20 Epochen an und gehe dann hoch auf +50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ead8b363-4113-49d7-9832-d4b311e8bd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ergebnisse wurden in 'fold_performance_model_cnn2_cvr.json' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# Convert NumPy arrays in histories to lists\n",
    "fold_histories_lists_model2 = [\n",
    "    {key: np.array(val).tolist() for key, val in fold_history.items()} \n",
    "    for fold_history in fold_histories_model_cnn2  # use the correct variable here\n",
    "]\n",
    "\n",
    "results = {\n",
    "    \"fold_performance\": fold_performance_model_cnn2,\n",
    "    \"average_performance\": avg_perf_model_cnn2,\n",
    "    \"fold_histories\": fold_histories_lists_model2\n",
    "}\n",
    "\n",
    "# Speichern der Ergebnisse\n",
    "save_results_to_json('fold_performance_model_cnn2_cvr.json', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "262bdfdd-76aa-46c8-bf0f-069547b6c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = 'fold_performance_model_cnn2_cvr.json'\n",
    "#full_path = os.path.join(default_path, file_name)\n",
    "\n",
    "#with open(full_path, 'r') as file:\n",
    "#    results = json.load(file)\n",
    "\n",
    "# Extract the histories\n",
    "#fold_histories = results['fold_histories']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca72461",
   "metadata": {},
   "source": [
    "## CNN3 - max pooling, stride 1,2 , padding = valid, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "db03cd1f-0108-49e3-a72b-2b6a7768e3e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_11 (Conv2D)          (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPooli  (None, 127, 127, 32)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPooli  (None, 62, 62, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 60, 60, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPooli  (None, 30, 30, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 14, 14, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               3211392   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 12)                1548      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3343116 (12.75 MB)\n",
      "Trainable params: 3343116 (12.75 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "388/388 [==============================] - 296s 762ms/step - loss: 1.5258 - accuracy: 0.4951 - val_loss: 1.2330 - val_accuracy: 0.6123\n",
      "Epoch 2/70\n",
      "388/388 [==============================] - 283s 729ms/step - loss: 1.1235 - accuracy: 0.6226 - val_loss: 1.0848 - val_accuracy: 0.6345\n",
      "Epoch 3/70\n",
      "388/388 [==============================] - 280s 720ms/step - loss: 0.9857 - accuracy: 0.6771 - val_loss: 1.0533 - val_accuracy: 0.6497\n",
      "Epoch 4/70\n",
      "388/388 [==============================] - 290s 746ms/step - loss: 0.8877 - accuracy: 0.7080 - val_loss: 1.1194 - val_accuracy: 0.6206\n",
      "Epoch 5/70\n",
      "388/388 [==============================] - 280s 722ms/step - loss: 0.8271 - accuracy: 0.7293 - val_loss: 0.8995 - val_accuracy: 0.7110\n",
      "Epoch 6/70\n",
      "388/388 [==============================] - 281s 723ms/step - loss: 0.7517 - accuracy: 0.7544 - val_loss: 0.8650 - val_accuracy: 0.7135\n",
      "Epoch 7/70\n",
      "388/388 [==============================] - 275s 708ms/step - loss: 0.6987 - accuracy: 0.7735 - val_loss: 0.9188 - val_accuracy: 0.7148\n",
      "Epoch 8/70\n",
      "388/388 [==============================] - 273s 702ms/step - loss: 0.6671 - accuracy: 0.7841 - val_loss: 0.9091 - val_accuracy: 0.7168\n",
      "Epoch 9/70\n",
      "388/388 [==============================] - 272s 699ms/step - loss: 0.6229 - accuracy: 0.7928 - val_loss: 0.9443 - val_accuracy: 0.7065\n",
      "Epoch 10/70\n",
      "388/388 [==============================] - 276s 711ms/step - loss: 0.5935 - accuracy: 0.8037 - val_loss: 0.8467 - val_accuracy: 0.7319\n",
      "Epoch 11/70\n",
      "388/388 [==============================] - 275s 708ms/step - loss: 0.5500 - accuracy: 0.8142 - val_loss: 0.8114 - val_accuracy: 0.7500\n",
      "Epoch 12/70\n",
      "388/388 [==============================] - 283s 729ms/step - loss: 0.5296 - accuracy: 0.8273 - val_loss: 0.8745 - val_accuracy: 0.7429\n",
      "Epoch 13/70\n",
      "388/388 [==============================] - 281s 723ms/step - loss: 0.4759 - accuracy: 0.8404 - val_loss: 0.8194 - val_accuracy: 0.7584\n",
      "Epoch 14/70\n",
      "388/388 [==============================] - 302s 777ms/step - loss: 0.4441 - accuracy: 0.8507 - val_loss: 0.8010 - val_accuracy: 0.7668\n",
      "Epoch 15/70\n",
      "388/388 [==============================] - 275s 709ms/step - loss: 0.4200 - accuracy: 0.8589 - val_loss: 0.8328 - val_accuracy: 0.7632\n",
      "Epoch 16/70\n",
      "388/388 [==============================] - 273s 703ms/step - loss: 0.4080 - accuracy: 0.8649 - val_loss: 0.7983 - val_accuracy: 0.7674\n",
      "Epoch 17/70\n",
      "388/388 [==============================] - 274s 705ms/step - loss: 0.3783 - accuracy: 0.8747 - val_loss: 0.7995 - val_accuracy: 0.7700\n",
      "Epoch 18/70\n",
      "388/388 [==============================] - 275s 708ms/step - loss: 0.3556 - accuracy: 0.8805 - val_loss: 0.8038 - val_accuracy: 0.7729\n",
      "Epoch 19/70\n",
      "388/388 [==============================] - 270s 696ms/step - loss: 0.3312 - accuracy: 0.8906 - val_loss: 0.8939 - val_accuracy: 0.7639\n",
      "Epoch 20/70\n",
      "388/388 [==============================] - 270s 695ms/step - loss: 0.3272 - accuracy: 0.8900 - val_loss: 0.8773 - val_accuracy: 0.7629\n",
      "Epoch 21/70\n",
      "388/388 [==============================] - 277s 714ms/step - loss: 0.3178 - accuracy: 0.8943 - val_loss: 0.8016 - val_accuracy: 0.7806\n",
      "Epoch 22/70\n",
      "388/388 [==============================] - 266s 684ms/step - loss: 0.2886 - accuracy: 0.9033 - val_loss: 0.9653 - val_accuracy: 0.7590\n",
      "Epoch 23/70\n",
      "388/388 [==============================] - 269s 693ms/step - loss: 0.2839 - accuracy: 0.9065 - val_loss: 0.8765 - val_accuracy: 0.7719\n",
      "Epoch 24/70\n",
      "388/388 [==============================] - 279s 717ms/step - loss: 0.2729 - accuracy: 0.9090 - val_loss: 0.9067 - val_accuracy: 0.7623\n",
      "Epoch 25/70\n",
      "388/388 [==============================] - 276s 710ms/step - loss: 0.2558 - accuracy: 0.9155 - val_loss: 0.8802 - val_accuracy: 0.7713\n",
      "Epoch 26/70\n",
      "388/388 [==============================] - 283s 728ms/step - loss: 0.2689 - accuracy: 0.9113 - val_loss: 0.8983 - val_accuracy: 0.7645\n",
      "Epoch 27/70\n",
      "388/388 [==============================] - 281s 723ms/step - loss: 0.2440 - accuracy: 0.9179 - val_loss: 0.9463 - val_accuracy: 0.7774\n",
      "Epoch 28/70\n",
      "388/388 [==============================] - 282s 726ms/step - loss: 0.2347 - accuracy: 0.9235 - val_loss: 0.8399 - val_accuracy: 0.7852\n",
      "Epoch 29/70\n",
      "388/388 [==============================] - 275s 707ms/step - loss: 0.2180 - accuracy: 0.9297 - val_loss: 0.9340 - val_accuracy: 0.7845\n",
      "Epoch 30/70\n",
      "388/388 [==============================] - 275s 709ms/step - loss: 0.2015 - accuracy: 0.9329 - val_loss: 1.0143 - val_accuracy: 0.7639\n",
      "Epoch 31/70\n",
      "388/388 [==============================] - 275s 708ms/step - loss: 0.2077 - accuracy: 0.9324 - val_loss: 0.8970 - val_accuracy: 0.7765\n",
      "Epoch 32/70\n",
      "388/388 [==============================] - 278s 717ms/step - loss: 0.2155 - accuracy: 0.9285 - val_loss: 0.9212 - val_accuracy: 0.7855\n",
      "Epoch 33/70\n",
      "388/388 [==============================] - 294s 757ms/step - loss: 0.1816 - accuracy: 0.9371 - val_loss: 1.0883 - val_accuracy: 0.7848\n",
      "Epoch 34/70\n",
      "388/388 [==============================] - 269s 693ms/step - loss: 0.1938 - accuracy: 0.9340 - val_loss: 1.0699 - val_accuracy: 0.7703\n",
      "Epoch 35/70\n",
      "388/388 [==============================] - 284s 732ms/step - loss: 0.1772 - accuracy: 0.9389 - val_loss: 1.0805 - val_accuracy: 0.7735\n",
      "Epoch 36/70\n",
      "388/388 [==============================] - 283s 727ms/step - loss: 0.1834 - accuracy: 0.9373 - val_loss: 0.9939 - val_accuracy: 0.7800\n",
      "Epoch 37/70\n",
      "388/388 [==============================] - 270s 696ms/step - loss: 0.1657 - accuracy: 0.9449 - val_loss: 0.9642 - val_accuracy: 0.7881\n",
      "Epoch 38/70\n",
      "388/388 [==============================] - 297s 766ms/step - loss: 0.1831 - accuracy: 0.9402 - val_loss: 0.9668 - val_accuracy: 0.7855\n",
      "Epoch 39/70\n",
      "388/388 [==============================] - 291s 749ms/step - loss: 0.1795 - accuracy: 0.9402 - val_loss: 0.9643 - val_accuracy: 0.7755\n",
      "Epoch 40/70\n",
      "388/388 [==============================] - 279s 718ms/step - loss: 0.1591 - accuracy: 0.9455 - val_loss: 1.0021 - val_accuracy: 0.7826\n",
      "Epoch 41/70\n",
      "388/388 [==============================] - 274s 705ms/step - loss: 0.1650 - accuracy: 0.9442 - val_loss: 0.9935 - val_accuracy: 0.7852\n",
      "Epoch 42/70\n",
      "388/388 [==============================] - 272s 701ms/step - loss: 0.1718 - accuracy: 0.9453 - val_loss: 1.0203 - val_accuracy: 0.7752\n",
      "Epoch 43/70\n",
      "388/388 [==============================] - 270s 696ms/step - loss: 0.1487 - accuracy: 0.9513 - val_loss: 0.9625 - val_accuracy: 0.7877\n",
      "Epoch 44/70\n",
      "388/388 [==============================] - 278s 717ms/step - loss: 0.1453 - accuracy: 0.9534 - val_loss: 1.0525 - val_accuracy: 0.7797\n",
      "Epoch 45/70\n",
      "388/388 [==============================] - 272s 699ms/step - loss: 0.1533 - accuracy: 0.9484 - val_loss: 1.0806 - val_accuracy: 0.7865\n",
      "Epoch 46/70\n",
      "388/388 [==============================] - 272s 701ms/step - loss: 0.1518 - accuracy: 0.9490 - val_loss: 1.0693 - val_accuracy: 0.7919\n",
      "Epoch 47/70\n",
      "388/388 [==============================] - 271s 699ms/step - loss: 0.1384 - accuracy: 0.9544 - val_loss: 1.0863 - val_accuracy: 0.7806\n",
      "Epoch 48/70\n",
      "388/388 [==============================] - 271s 698ms/step - loss: 0.1440 - accuracy: 0.9529 - val_loss: 1.1124 - val_accuracy: 0.7865\n",
      "Epoch 49/70\n",
      "388/388 [==============================] - 271s 698ms/step - loss: 0.1576 - accuracy: 0.9481 - val_loss: 1.0594 - val_accuracy: 0.7877\n",
      "Epoch 50/70\n",
      "388/388 [==============================] - 290s 746ms/step - loss: 0.1361 - accuracy: 0.9542 - val_loss: 1.2200 - val_accuracy: 0.7797\n",
      "Epoch 51/70\n",
      "388/388 [==============================] - 285s 733ms/step - loss: 0.1505 - accuracy: 0.9540 - val_loss: 1.0877 - val_accuracy: 0.7806\n",
      "Epoch 52/70\n",
      "388/388 [==============================] - 276s 710ms/step - loss: 0.1251 - accuracy: 0.9580 - val_loss: 1.1360 - val_accuracy: 0.7852\n",
      "Epoch 53/70\n",
      "388/388 [==============================] - 276s 710ms/step - loss: 0.1314 - accuracy: 0.9584 - val_loss: 1.0778 - val_accuracy: 0.7916\n",
      "Epoch 54/70\n",
      "388/388 [==============================] - 276s 710ms/step - loss: 0.1239 - accuracy: 0.9600 - val_loss: 1.1218 - val_accuracy: 0.7968\n",
      "Epoch 55/70\n",
      "388/388 [==============================] - 278s 717ms/step - loss: 0.1351 - accuracy: 0.9560 - val_loss: 1.0206 - val_accuracy: 0.7910\n",
      "Epoch 56/70\n",
      "388/388 [==============================] - 287s 738ms/step - loss: 0.1316 - accuracy: 0.9567 - val_loss: 1.1821 - val_accuracy: 0.7829\n",
      "Epoch 57/70\n",
      "388/388 [==============================] - 282s 726ms/step - loss: 0.1180 - accuracy: 0.9617 - val_loss: 1.1117 - val_accuracy: 0.7877\n",
      "Epoch 58/70\n",
      "388/388 [==============================] - 279s 717ms/step - loss: 0.1332 - accuracy: 0.9566 - val_loss: 1.1768 - val_accuracy: 0.7858\n",
      "Epoch 59/70\n",
      "388/388 [==============================] - 280s 720ms/step - loss: 0.1334 - accuracy: 0.9569 - val_loss: 1.0645 - val_accuracy: 0.7839\n",
      "Epoch 60/70\n",
      "388/388 [==============================] - 278s 716ms/step - loss: 0.1125 - accuracy: 0.9627 - val_loss: 1.1756 - val_accuracy: 0.7929\n",
      "Epoch 61/70\n",
      "388/388 [==============================] - 280s 722ms/step - loss: 0.1254 - accuracy: 0.9610 - val_loss: 1.1260 - val_accuracy: 0.7887\n",
      "Epoch 62/70\n",
      "388/388 [==============================] - 280s 722ms/step - loss: 0.1154 - accuracy: 0.9628 - val_loss: 1.2407 - val_accuracy: 0.7748\n",
      "Epoch 63/70\n",
      "388/388 [==============================] - 285s 734ms/step - loss: 0.1084 - accuracy: 0.9647 - val_loss: 1.3005 - val_accuracy: 0.7971\n",
      "Epoch 64/70\n",
      "388/388 [==============================] - 276s 711ms/step - loss: 0.1305 - accuracy: 0.9573 - val_loss: 1.2219 - val_accuracy: 0.7823\n",
      "Epoch 65/70\n",
      "388/388 [==============================] - 287s 739ms/step - loss: 0.1060 - accuracy: 0.9668 - val_loss: 1.1044 - val_accuracy: 0.7948\n",
      "Epoch 66/70\n",
      "388/388 [==============================] - 319s 820ms/step - loss: 0.1257 - accuracy: 0.9606 - val_loss: 1.1718 - val_accuracy: 0.7900\n",
      "Epoch 67/70\n",
      "388/388 [==============================] - 280s 721ms/step - loss: 0.1122 - accuracy: 0.9640 - val_loss: 1.1878 - val_accuracy: 0.7800\n",
      "Epoch 68/70\n",
      "388/388 [==============================] - 283s 729ms/step - loss: 0.1163 - accuracy: 0.9634 - val_loss: 1.1507 - val_accuracy: 0.7719\n",
      "Epoch 69/70\n",
      "388/388 [==============================] - 274s 706ms/step - loss: 0.1064 - accuracy: 0.9631 - val_loss: 1.1734 - val_accuracy: 0.7884\n",
      "Epoch 70/70\n",
      "388/388 [==============================] - 279s 719ms/step - loss: 0.1039 - accuracy: 0.9678 - val_loss: 1.2527 - val_accuracy: 0.7723\n"
     ]
    }
   ],
   "source": [
    "# Set the path for the model\n",
    "model_path = '/Users/linuszarse/Documents/UNI/Master-Uni Potsdam/3. Semester/Machine Learning 2/Klausurprojekt/Best_Models/'\n",
    "model_filename = 'best_model_cnn3.keras'\n",
    "checkpoint_path = os.path.join(model_path, model_filename)\n",
    "\n",
    "# Check if the model exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    # Load the model\n",
    "    model_cnn3 = load_model(checkpoint_path)\n",
    "else:\n",
    "    model_cnn3 = tf.keras.models.Sequential([\n",
    "    Conv2D(32 , (3,3) , strides = 1 , padding = 'valid' , activation = 'relu', input_shape=(256, 256, 3)),\n",
    "    MaxPooling2D((2,2) , strides = 2 , padding = 'valid'),\n",
    "\n",
    "    Conv2D(64 , (3,3) , strides = 1 , padding = 'valid' , activation = 'relu'),\n",
    "    MaxPooling2D((2,2) , strides = 2 , padding = 'valid'),\n",
    "    \n",
    "    Conv2D(64 , (3,3) , strides = 1 , padding = 'valid' , activation = 'relu'),\n",
    "    MaxPooling2D((2,2) , strides = 2 , padding = 'valid'),\n",
    "    \n",
    "    Conv2D(128 , (3,3) , strides = 1 , padding = 'valid' , activation = 'relu'),\n",
    "    MaxPooling2D((2,2) , strides = 2 , padding = 'valid'),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(units = 128 , activation = 'relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(units = num_classes, activation='softmax')  # Adjust 'num_classes' based on your task\n",
    "])\n",
    "\n",
    "    # Compile the model\n",
    "    model_cnn3.compile(optimizer=tf.keras.optimizers.legacy.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Display the model summary\n",
    "    model_cnn3.summary()\n",
    "\n",
    "    #checkpoint = ModelCheckpoint(\"best_model.keras\", monitor='val_loss', save_best_only=True)\n",
    "    #checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True)\n",
    "    \n",
    "    # Define a callback to save the best model during training\n",
    "    #checkpoint = ModelCheckpoint(os.path.join(model_path, 'best_model_cnn2.keras'), monitor='val_loss', save_best_only=True)\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "    # Train the model\n",
    "    history_cnn3 = model_cnn3.fit(\n",
    "        train_generator,\n",
    "        epochs=50,  # Adjust the number of epochs based on your needs\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[checkpoint],\n",
    "        class_weight=class_weights_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b74fe335-d54c-4613-9ff5-8b12e7060508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright. Model is already compiled and trained\n"
     ]
    }
   ],
   "source": [
    "check_compile(model_cnn3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "abeb057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = '/Users/linuszarse/Documents/UNI/Master-Uni Potsdam/3. Semester/Machine Learning 2/Klausurprojekt/Best_Models/'\n",
    "history_filename_cnn3 = 'best_model_cnn3_training_history.json'\n",
    "\n",
    "# Speichern der Historie\n",
    "save_history_to_json(history_cnn3, history_filename_cnn3, history_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "84e23405-f1f5-4b2f-8d27-48eb98992c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern der Daten in einer JSON-Datei\n",
    "with open(os.path.join(model_path, history_filename_cnn3), 'r') as file:\n",
    "    loaded_history = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6f4eb-dd3d-4a5f-8565-4660174adf59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb5d7fd8-42f8-461d-9e38-f65a51a3b330",
   "metadata": {},
   "source": [
    "***\n",
    "### Evalutation Protocoll: K-Fold Cross Validation for CNN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "08bd4628-9462-4c38-91b8-043a176d3b4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9932 validated image filenames belonging to 12 classes.\n",
      "Found 2483 validated image filenames belonging to 12 classes.\n",
      "Epoch 1/20\n",
      "310/310 [==============================] - 214s 689ms/step - loss: 2.3621 - accuracy: 0.3279 - val_loss: 2.1772 - val_accuracy: 0.3657\n",
      "Epoch 2/20\n",
      "310/310 [==============================] - 201s 648ms/step - loss: 2.1826 - accuracy: 0.3604 - val_loss: 2.0193 - val_accuracy: 0.3795\n",
      "Epoch 3/20\n",
      "310/310 [==============================] - 213s 686ms/step - loss: 2.0868 - accuracy: 0.3736 - val_loss: 1.9988 - val_accuracy: 0.3799\n",
      "Epoch 4/20\n",
      "310/310 [==============================] - 210s 679ms/step - loss: 1.9856 - accuracy: 0.3819 - val_loss: 1.8869 - val_accuracy: 0.3900\n",
      "Epoch 5/20\n",
      "310/310 [==============================] - 641s 2s/step - loss: 1.8507 - accuracy: 0.3908 - val_loss: 1.5879 - val_accuracy: 0.4748\n",
      "Epoch 6/20\n",
      "310/310 [==============================] - 3767s 12s/step - loss: 1.6111 - accuracy: 0.4634 - val_loss: 1.9902 - val_accuracy: 0.3304\n",
      "Epoch 7/20\n",
      "310/310 [==============================] - 226s 728ms/step - loss: 1.5075 - accuracy: 0.4927 - val_loss: 1.3495 - val_accuracy: 0.5455\n",
      "Epoch 8/20\n",
      "310/310 [==============================] - 252s 810ms/step - loss: 1.2916 - accuracy: 0.5617 - val_loss: 1.3153 - val_accuracy: 0.5621\n",
      "Epoch 9/20\n",
      "310/310 [==============================] - 221s 712ms/step - loss: 1.1828 - accuracy: 0.5935 - val_loss: 1.1589 - val_accuracy: 0.6067\n",
      "Epoch 10/20\n",
      "310/310 [==============================] - 197s 635ms/step - loss: 1.0520 - accuracy: 0.6346 - val_loss: 1.2217 - val_accuracy: 0.6209\n",
      "Epoch 11/20\n",
      "310/310 [==============================] - 203s 654ms/step - loss: 0.9305 - accuracy: 0.6802 - val_loss: 1.1386 - val_accuracy: 0.6416\n",
      "Epoch 12/20\n",
      "310/310 [==============================] - 206s 665ms/step - loss: 0.8156 - accuracy: 0.7206 - val_loss: 1.1089 - val_accuracy: 0.6571\n",
      "Epoch 13/20\n",
      "310/310 [==============================] - 205s 662ms/step - loss: 0.7131 - accuracy: 0.7558 - val_loss: 1.2134 - val_accuracy: 0.6627\n",
      "Epoch 14/20\n",
      "310/310 [==============================] - 225s 724ms/step - loss: 0.6163 - accuracy: 0.7872 - val_loss: 1.1298 - val_accuracy: 0.6818\n",
      "Epoch 15/20\n",
      "310/310 [==============================] - 220s 711ms/step - loss: 0.5127 - accuracy: 0.8278 - val_loss: 1.2274 - val_accuracy: 0.6855\n",
      "Epoch 16/20\n",
      "310/310 [==============================] - 210s 678ms/step - loss: 0.4529 - accuracy: 0.8459 - val_loss: 1.3273 - val_accuracy: 0.6851\n",
      "Epoch 17/20\n",
      "310/310 [==============================] - 204s 657ms/step - loss: 0.4046 - accuracy: 0.8655 - val_loss: 1.3875 - val_accuracy: 0.6851\n",
      "Epoch 18/20\n",
      "310/310 [==============================] - 205s 661ms/step - loss: 0.3348 - accuracy: 0.8856 - val_loss: 1.6584 - val_accuracy: 0.6968\n",
      "Epoch 19/20\n",
      "310/310 [==============================] - 204s 657ms/step - loss: 0.2780 - accuracy: 0.9024 - val_loss: 1.5718 - val_accuracy: 0.7123\n",
      "Epoch 20/20\n",
      "310/310 [==============================] - 233s 752ms/step - loss: 0.2444 - accuracy: 0.9147 - val_loss: 1.5919 - val_accuracy: 0.7119\n",
      "Found 9932 validated image filenames belonging to 12 classes.\n",
      "Found 2483 validated image filenames belonging to 12 classes.\n",
      "Epoch 1/20\n",
      "310/310 [==============================] - 209s 673ms/step - loss: 2.3031 - accuracy: 0.3299 - val_loss: 2.4634 - val_accuracy: 0.3701\n",
      "Epoch 2/20\n",
      "310/310 [==============================] - 247s 796ms/step - loss: 2.1270 - accuracy: 0.3571 - val_loss: 1.8820 - val_accuracy: 0.3856\n",
      "Epoch 3/20\n",
      "310/310 [==============================] - 237s 765ms/step - loss: 1.8897 - accuracy: 0.3844 - val_loss: 1.7697 - val_accuracy: 0.4002\n",
      "Epoch 4/20\n",
      "310/310 [==============================] - 230s 743ms/step - loss: 1.7924 - accuracy: 0.4207 - val_loss: 1.5636 - val_accuracy: 0.4619\n",
      "Epoch 5/20\n",
      "310/310 [==============================] - 227s 730ms/step - loss: 1.6247 - accuracy: 0.4628 - val_loss: 1.4498 - val_accuracy: 0.5345\n",
      "Epoch 6/20\n",
      "310/310 [==============================] - 210s 677ms/step - loss: 1.4365 - accuracy: 0.5180 - val_loss: 1.3750 - val_accuracy: 0.5450\n",
      "Epoch 7/20\n",
      "310/310 [==============================] - 220s 709ms/step - loss: 1.3357 - accuracy: 0.5468 - val_loss: 1.2064 - val_accuracy: 0.5954\n",
      "Epoch 8/20\n",
      "310/310 [==============================] - 219s 706ms/step - loss: 1.1649 - accuracy: 0.6008 - val_loss: 1.1656 - val_accuracy: 0.6144\n",
      "Epoch 9/20\n",
      "310/310 [==============================] - 214s 691ms/step - loss: 1.1049 - accuracy: 0.6261 - val_loss: 1.0725 - val_accuracy: 0.6469\n",
      "Epoch 10/20\n",
      "310/310 [==============================] - 227s 731ms/step - loss: 0.9758 - accuracy: 0.6679 - val_loss: 1.0029 - val_accuracy: 0.6826\n",
      "Epoch 11/20\n",
      "310/310 [==============================] - 222s 715ms/step - loss: 0.8446 - accuracy: 0.7043 - val_loss: 1.1143 - val_accuracy: 0.6603\n",
      "Epoch 12/20\n",
      "310/310 [==============================] - 208s 670ms/step - loss: 0.7248 - accuracy: 0.7520 - val_loss: 1.0486 - val_accuracy: 0.6867\n",
      "Epoch 13/20\n",
      "310/310 [==============================] - 201s 649ms/step - loss: 0.6203 - accuracy: 0.7870 - val_loss: 1.0248 - val_accuracy: 0.7074\n",
      "Epoch 14/20\n",
      "310/310 [==============================] - 204s 657ms/step - loss: 0.5625 - accuracy: 0.8125 - val_loss: 1.1786 - val_accuracy: 0.6855\n",
      "Epoch 15/20\n",
      "310/310 [==============================] - 207s 666ms/step - loss: 0.4579 - accuracy: 0.8429 - val_loss: 1.3345 - val_accuracy: 0.6997\n",
      "Epoch 16/20\n",
      "310/310 [==============================] - 226s 729ms/step - loss: 0.3663 - accuracy: 0.8732 - val_loss: 1.2806 - val_accuracy: 0.7041\n",
      "Epoch 17/20\n",
      "310/310 [==============================] - 213s 686ms/step - loss: 0.2956 - accuracy: 0.8971 - val_loss: 1.5939 - val_accuracy: 0.7005\n",
      "Epoch 18/20\n",
      "310/310 [==============================] - 211s 682ms/step - loss: 0.2463 - accuracy: 0.9144 - val_loss: 1.6397 - val_accuracy: 0.6936\n",
      "Epoch 19/20\n",
      "310/310 [==============================] - 207s 667ms/step - loss: 0.2058 - accuracy: 0.9293 - val_loss: 1.7390 - val_accuracy: 0.7090\n",
      "Epoch 20/20\n",
      "310/310 [==============================] - 214s 691ms/step - loss: 0.1760 - accuracy: 0.9409 - val_loss: 1.7971 - val_accuracy: 0.6887\n",
      "Found 9932 validated image filenames belonging to 12 classes.\n",
      "Found 2483 validated image filenames belonging to 12 classes.\n",
      "Epoch 1/20\n",
      "310/310 [==============================] - 216s 695ms/step - loss: 2.3216 - accuracy: 0.3196 - val_loss: 2.1239 - val_accuracy: 0.3454\n",
      "Epoch 2/20\n",
      "310/310 [==============================] - 221s 711ms/step - loss: 2.0691 - accuracy: 0.3673 - val_loss: 1.7862 - val_accuracy: 0.4647\n",
      "Epoch 3/20\n",
      "310/310 [==============================] - 205s 661ms/step - loss: 1.7757 - accuracy: 0.4292 - val_loss: 1.5692 - val_accuracy: 0.4821\n",
      "Epoch 4/20\n",
      "310/310 [==============================] - 200s 644ms/step - loss: 1.5698 - accuracy: 0.4780 - val_loss: 1.4101 - val_accuracy: 0.5495\n",
      "Epoch 5/20\n",
      "310/310 [==============================] - 209s 674ms/step - loss: 1.4110 - accuracy: 0.5207 - val_loss: 1.4399 - val_accuracy: 0.5077\n",
      "Epoch 6/20\n",
      "310/310 [==============================] - 234s 755ms/step - loss: 1.3647 - accuracy: 0.5518 - val_loss: 1.3755 - val_accuracy: 0.5548\n",
      "Epoch 7/20\n",
      "310/310 [==============================] - 241s 777ms/step - loss: 1.2156 - accuracy: 0.5859 - val_loss: 1.1203 - val_accuracy: 0.6412\n",
      "Epoch 8/20\n",
      "310/310 [==============================] - 253s 817ms/step - loss: 1.0582 - accuracy: 0.6475 - val_loss: 1.0834 - val_accuracy: 0.6412\n",
      "Epoch 9/20\n",
      "310/310 [==============================] - 234s 754ms/step - loss: 0.9865 - accuracy: 0.6691 - val_loss: 1.0293 - val_accuracy: 0.6802\n",
      "Epoch 10/20\n",
      "310/310 [==============================] - 223s 720ms/step - loss: 0.8713 - accuracy: 0.7005 - val_loss: 0.9796 - val_accuracy: 0.6993\n",
      "Epoch 11/20\n",
      "310/310 [==============================] - 231s 744ms/step - loss: 0.7052 - accuracy: 0.7630 - val_loss: 1.0940 - val_accuracy: 0.6847\n",
      "Epoch 12/20\n",
      "310/310 [==============================] - 210s 678ms/step - loss: 0.6030 - accuracy: 0.7894 - val_loss: 0.9985 - val_accuracy: 0.7058\n",
      "Epoch 13/20\n",
      "310/310 [==============================] - 266s 859ms/step - loss: 0.4870 - accuracy: 0.8331 - val_loss: 1.0612 - val_accuracy: 0.7220\n",
      "Epoch 14/20\n",
      "310/310 [==============================] - 247s 798ms/step - loss: 0.4027 - accuracy: 0.8622 - val_loss: 1.1272 - val_accuracy: 0.7212\n",
      "Epoch 15/20\n",
      "310/310 [==============================] - 224s 723ms/step - loss: 0.3402 - accuracy: 0.8845 - val_loss: 1.2130 - val_accuracy: 0.7220\n",
      "Epoch 16/20\n",
      "310/310 [==============================] - 227s 731ms/step - loss: 0.2736 - accuracy: 0.9087 - val_loss: 1.3298 - val_accuracy: 0.7204\n",
      "Epoch 17/20\n",
      "310/310 [==============================] - 269s 866ms/step - loss: 0.2217 - accuracy: 0.9271 - val_loss: 1.4902 - val_accuracy: 0.7110\n",
      "Epoch 18/20\n",
      "310/310 [==============================] - 253s 816ms/step - loss: 0.1821 - accuracy: 0.9412 - val_loss: 1.4964 - val_accuracy: 0.7102\n",
      "Epoch 19/20\n",
      "310/310 [==============================] - 12060s 39s/step - loss: 0.1952 - accuracy: 0.9325 - val_loss: 1.5126 - val_accuracy: 0.7220\n",
      "Epoch 20/20\n",
      "310/310 [==============================] - 284s 915ms/step - loss: 0.1643 - accuracy: 0.9461 - val_loss: 1.7061 - val_accuracy: 0.7212\n",
      "Found 9932 validated image filenames belonging to 12 classes.\n",
      "Found 2483 validated image filenames belonging to 12 classes.\n",
      "Epoch 1/20\n",
      "310/310 [==============================] - 230s 742ms/step - loss: 2.2952 - accuracy: 0.3392 - val_loss: 2.2654 - val_accuracy: 0.3429\n",
      "Epoch 2/20\n",
      "310/310 [==============================] - 236s 762ms/step - loss: 2.1107 - accuracy: 0.3556 - val_loss: 1.7975 - val_accuracy: 0.3920\n",
      "Epoch 3/20\n",
      "310/310 [==============================] - 232s 748ms/step - loss: 1.8423 - accuracy: 0.3955 - val_loss: 1.7963 - val_accuracy: 0.3949\n",
      "Epoch 4/20\n",
      "310/310 [==============================] - 215s 694ms/step - loss: 1.6467 - accuracy: 0.4474 - val_loss: 1.5767 - val_accuracy: 0.4651\n",
      "Epoch 5/20\n",
      "310/310 [==============================] - 212s 684ms/step - loss: 1.5378 - accuracy: 0.4832 - val_loss: 1.3758 - val_accuracy: 0.5479\n",
      "Epoch 6/20\n",
      "310/310 [==============================] - 216s 696ms/step - loss: 1.4424 - accuracy: 0.5199 - val_loss: 1.4012 - val_accuracy: 0.5394\n",
      "Epoch 7/20\n",
      "310/310 [==============================] - 213s 685ms/step - loss: 1.3495 - accuracy: 0.5553 - val_loss: 1.2831 - val_accuracy: 0.5909\n",
      "Epoch 8/20\n",
      "310/310 [==============================] - 215s 693ms/step - loss: 1.2270 - accuracy: 0.5949 - val_loss: 1.2136 - val_accuracy: 0.6124\n",
      "Epoch 9/20\n",
      "310/310 [==============================] - 212s 682ms/step - loss: 1.1252 - accuracy: 0.6239 - val_loss: 1.0967 - val_accuracy: 0.6595\n",
      "Epoch 10/20\n",
      "310/310 [==============================] - 218s 703ms/step - loss: 0.9969 - accuracy: 0.6663 - val_loss: 1.1284 - val_accuracy: 0.6368\n",
      "Epoch 11/20\n",
      "310/310 [==============================] - 214s 691ms/step - loss: 0.8954 - accuracy: 0.6973 - val_loss: 0.9923 - val_accuracy: 0.6964\n",
      "Epoch 12/20\n",
      "310/310 [==============================] - 214s 689ms/step - loss: 0.7995 - accuracy: 0.7306 - val_loss: 1.0389 - val_accuracy: 0.6761\n",
      "Epoch 13/20\n",
      "310/310 [==============================] - 214s 688ms/step - loss: 0.6743 - accuracy: 0.7733 - val_loss: 1.1034 - val_accuracy: 0.7037\n",
      "Epoch 14/20\n",
      "310/310 [==============================] - 222s 717ms/step - loss: 0.5334 - accuracy: 0.8182 - val_loss: 1.0576 - val_accuracy: 0.7277\n",
      "Epoch 15/20\n",
      "310/310 [==============================] - 212s 685ms/step - loss: 0.4548 - accuracy: 0.8465 - val_loss: 1.0942 - val_accuracy: 0.7342\n",
      "Epoch 16/20\n",
      "310/310 [==============================] - 224s 722ms/step - loss: 0.3775 - accuracy: 0.8713 - val_loss: 1.2173 - val_accuracy: 0.7321\n",
      "Epoch 17/20\n",
      "310/310 [==============================] - 229s 739ms/step - loss: 0.3173 - accuracy: 0.8920 - val_loss: 1.3190 - val_accuracy: 0.7094\n",
      "Epoch 18/20\n",
      "310/310 [==============================] - 240s 774ms/step - loss: 0.2926 - accuracy: 0.8999 - val_loss: 1.4418 - val_accuracy: 0.7248\n",
      "Epoch 19/20\n",
      "310/310 [==============================] - 253s 817ms/step - loss: 0.2425 - accuracy: 0.9162 - val_loss: 1.4332 - val_accuracy: 0.7407\n",
      "Epoch 20/20\n",
      "310/310 [==============================] - 251s 808ms/step - loss: 0.2065 - accuracy: 0.9289 - val_loss: 1.4306 - val_accuracy: 0.7423\n",
      "Found 9932 validated image filenames belonging to 12 classes.\n",
      "Found 2483 validated image filenames belonging to 12 classes.\n",
      "Epoch 1/20\n",
      "310/310 [==============================] - 230s 741ms/step - loss: 2.2276 - accuracy: 0.3498 - val_loss: 1.8531 - val_accuracy: 0.3860\n",
      "Epoch 2/20\n",
      "310/310 [==============================] - 233s 752ms/step - loss: 1.9114 - accuracy: 0.3860 - val_loss: 2.0739 - val_accuracy: 0.3421\n",
      "Epoch 3/20\n",
      "310/310 [==============================] - 226s 730ms/step - loss: 1.7413 - accuracy: 0.4208 - val_loss: 1.5124 - val_accuracy: 0.4850\n",
      "Epoch 4/20\n",
      "310/310 [==============================] - 238s 767ms/step - loss: 1.5310 - accuracy: 0.4860 - val_loss: 1.3561 - val_accuracy: 0.5272\n",
      "Epoch 5/20\n",
      "310/310 [==============================] - 220s 708ms/step - loss: 1.4465 - accuracy: 0.5135 - val_loss: 1.2812 - val_accuracy: 0.5751\n",
      "Epoch 6/20\n",
      "310/310 [==============================] - 242s 782ms/step - loss: 1.2793 - accuracy: 0.5685 - val_loss: 1.1222 - val_accuracy: 0.6234\n",
      "Epoch 7/20\n",
      "310/310 [==============================] - 235s 759ms/step - loss: 1.1527 - accuracy: 0.6097 - val_loss: 1.2482 - val_accuracy: 0.6242\n",
      "Epoch 8/20\n",
      "310/310 [==============================] - 230s 741ms/step - loss: 1.0749 - accuracy: 0.6449 - val_loss: 1.0082 - val_accuracy: 0.6834\n",
      "Epoch 9/20\n",
      "310/310 [==============================] - 225s 724ms/step - loss: 0.9381 - accuracy: 0.6862 - val_loss: 1.0147 - val_accuracy: 0.6700\n",
      "Epoch 10/20\n",
      "310/310 [==============================] - 236s 763ms/step - loss: 0.7963 - accuracy: 0.7351 - val_loss: 1.0001 - val_accuracy: 0.6806\n",
      "Epoch 11/20\n",
      "310/310 [==============================] - 242s 782ms/step - loss: 0.7248 - accuracy: 0.7563 - val_loss: 0.9807 - val_accuracy: 0.7037\n",
      "Epoch 12/20\n",
      "310/310 [==============================] - 236s 760ms/step - loss: 0.6084 - accuracy: 0.7990 - val_loss: 1.1391 - val_accuracy: 0.7066\n",
      "Epoch 13/20\n",
      "310/310 [==============================] - 238s 768ms/step - loss: 0.4918 - accuracy: 0.8337 - val_loss: 1.0623 - val_accuracy: 0.7163\n",
      "Epoch 14/20\n",
      "310/310 [==============================] - 239s 772ms/step - loss: 0.4051 - accuracy: 0.8629 - val_loss: 1.1550 - val_accuracy: 0.7289\n",
      "Epoch 15/20\n",
      "310/310 [==============================] - 245s 789ms/step - loss: 0.3559 - accuracy: 0.8819 - val_loss: 1.2202 - val_accuracy: 0.7281\n",
      "Epoch 16/20\n",
      "310/310 [==============================] - 252s 812ms/step - loss: 0.2676 - accuracy: 0.9094 - val_loss: 1.3030 - val_accuracy: 0.7370\n",
      "Epoch 17/20\n",
      "310/310 [==============================] - 226s 730ms/step - loss: 0.2316 - accuracy: 0.9201 - val_loss: 1.4959 - val_accuracy: 0.7183\n",
      "Epoch 18/20\n",
      "310/310 [==============================] - 241s 776ms/step - loss: 0.2139 - accuracy: 0.9275 - val_loss: 1.3108 - val_accuracy: 0.7293\n",
      "Epoch 19/20\n",
      "310/310 [==============================] - 222s 714ms/step - loss: 0.1817 - accuracy: 0.9375 - val_loss: 1.4876 - val_accuracy: 0.7354\n",
      "Epoch 20/20\n",
      "310/310 [==============================] - 224s 721ms/step - loss: 0.1534 - accuracy: 0.9468 - val_loss: 1.5685 - val_accuracy: 0.7354\n"
     ]
    }
   ],
   "source": [
    "fold_performance_model_cnn3, avg_perf_model_cnn3, fold_histories_model_cnn3 = k_fold_cross_validation(model_cnn3, \n",
    "                                                                                                      train_generator, \n",
    "                                                                                                      n_splits=5, \n",
    "                                                                                                      epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "84704e7f-8d97-4188-8790-835e1080dfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ergebnisse wurden in 'fold_performance_model_cnn3_cvr.json' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# Convert NumPy arrays in histories to lists\n",
    "fold_histories_lists_model3 = [\n",
    "    {key: np.array(val).tolist() for key, val in fold_history.items()} \n",
    "    for fold_history in fold_histories_model_cnn3  # use the correct variable here\n",
    "]\n",
    "\n",
    "results_cnn3 = {\n",
    "    \"fold_performance\": fold_performance_model_cnn3,\n",
    "    \"average_performance\": avg_perf_model_cnn3,\n",
    "    \"fold_histories\": fold_histories_lists_model3\n",
    "}\n",
    "\n",
    "# Speichern der Ergebnisse\n",
    "save_results_to_json('fold_performance_model_cnn3_cvr.json', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4b01467a-a567-4259-8abf-c018411ce066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = 'fold_performance_model_cnn3_cvr.json'\n",
    "#full_path = os.path.join(default_path, file_name)\n",
    "\n",
    "#with open(full_path, 'r') as file:\n",
    "#    results_cnn3 = json.load(file)\n",
    "\n",
    "# Extract the histories\n",
    "#fold_histories = results_cnn3['fold_histories']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc370739",
   "metadata": {},
   "source": [
    "***\n",
    "## Hyperparameter Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7302a9e-9317-4570-b5b4-6d72c18d01a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11d4d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Tuning the number and size of convolutional layers\n",
    "    for i in range(hp.Int('num_conv_layers', 1, 3)):\n",
    "        model.add(Conv2D(hp.Int(f'filters_{i}', 32, 128, 32), (3, 3), activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Tuning the number and size of dense layers\n",
    "    for i in range(hp.Int('num_dense_layers', 1, 2)):\n",
    "        model.add(Dense(hp.Int(f'units_{i}', 128, 512, 128), activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(hp.Float(f'dropout_{i}', 0.2, 0.5, 0.1)))\n",
    "        # Adding L1/L2 regularization\n",
    "        model.add(Dense(units=hp.Int(f'units_{i}', 128, 512, 128), \n",
    "                        activation='relu', \n",
    "                        kernel_regularizer=l1_l2(l1=hp.Float('l1', 1e-5, 1e-2, sampling='LOG', default=1e-5), \n",
    "                                                 l2=hp.Float('l2', 1e-5, 1e-2, sampling='LOG', default=1e-4))))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Tuning the learning rate\n",
    "    hp_lr = hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=hp_lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "933d90b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [04h 10m 19s]\n",
      "val_accuracy: 0.7599999904632568\n",
      "\n",
      "Best val_accuracy So Far: 0.8135483860969543\n",
      "Total elapsed time: 15h 44m 36s\n"
     ]
    }
   ],
   "source": [
    "# Bayesian Optimization tuner\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    num_initial_points=4,\n",
    "    directory='bayesian_optimization',\n",
    "    project_name='garbage_classification'\n",
    ")\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "tuner.search(train_generator, epochs=50, \n",
    "             validation_data=validation_generator, \n",
    "             callbacks=[early_stopping],\n",
    "             workers=6,\n",
    "            use_multiprocessing=False,)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "\n",
    "# Build the model with the best hyperparameters\n",
    "hyperparameter_model = build_model(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff4b13c9-92be-4a14-8a8f-4638bb0e3995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 254, 254, 32)      128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 127, 127, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 125, 125, 128)     36992     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 125, 125, 128)     512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 62, 62, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 60, 60, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 60, 60, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 30, 30, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 115200)            0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 384)               44237184  \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 384)               1536      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 384)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 384)               147840    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               49280     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 12)                1548      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44641036 (170.29 MB)\n",
      "Trainable params: 44639436 (170.29 MB)\n",
      "Non-trainable params: 1600 (6.25 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef6ff2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainieren und speichern des besten Modells\n",
    "model_path = '/Users/linuszarse/Documents/UNI/Master-Uni Potsdam/3. Semester/Machine Learning 2/Klausurprojekt/Best_Models/'\n",
    "model_filename_hp = 'hyperparameter_model.keras'\n",
    "checkpoint_path = os.path.join(model_path, model_filename)\n",
    "\n",
    "hyperparameter_model.compile(optimizer=tf.keras.optimizers.legacy.Adam(), \n",
    "                             loss='categorical_crossentropy', \n",
    "                             metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, \n",
    "                             monitor='val_loss', \n",
    "                             save_best_only=True, \n",
    "                             save_weights_only=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "history_hp = hyperparameter_model.fit(train_generator, \n",
    "                                   epochs=50, \n",
    "                                   validation_data=validation_generator, \n",
    "                                   callbacks=[checkpoint],\n",
    "                                   class_weight=class_weights_dict)\n",
    "\n",
    "\n",
    "#hyperparameter_model.save(checkpoint_path) -> bereits gespeichert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f161e96b-e542-43ef-8483-c9654da51fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern der Trainingshistorie\n",
    "history_filename_hp = 'best_model_hp_model_training_history.json'\n",
    "save_history_to_json(history_hp, history_filename_hp, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360947e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_path, history_filename_hp), 'r') as file:\n",
    "    loaded_history = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91cf8de-e326-47c2-9e07-0deab17e85e5",
   "metadata": {},
   "source": [
    "***\n",
    "### Evalutation Protocoll: K-Fold Cross Validation for Hyperparameter Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041d2a2-cdc0-41f8-af43-4f80e16ba169",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_performance_hp_model, avg_perf_hp_model, fold_histories_hp_model = k_fold_cross_validation(hyperparameter_model, \n",
    "                                                                                                      train_generator, \n",
    "                                                                                                      n_splits=5, \n",
    "                                                                                                      epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313dbb2d-144c-4f12-ae38-5b0c7bd6411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays in histories to lists\n",
    "fold_histories_lists_hp_model = [\n",
    "    {key: np.array(val).tolist() for key, val in fold_history.items()} \n",
    "    for fold_history in fold_histories_hp_model  # use the correct variable here\n",
    "]\n",
    "\n",
    "results_hp = {\n",
    "    \"fold_performance\": fold_performance_hp_model,\n",
    "    \"average_performance\": avg_perf_hp_model,\n",
    "    \"fold_histories\": fold_histories_hp_model\n",
    "}\n",
    "\n",
    "# Speichern der Ergebnisse\n",
    "save_results_to_json('fold_performance_hp_model_cvr.json', results_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3af3a6-39d4-45ff-b861-3353d390d4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dd4821d",
   "metadata": {},
   "source": [
    "***\n",
    "# Fine-Tuning of Pre-Trained Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0872c81-409b-48bb-b91e-f448360f9997",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b209b-fa56-4155-b0d2-275030dc3c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a GPU is available\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"No GPU found. Please make sure you have a GPU and TensorFlow is configured to use it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c353b7-9980-48aa-a06d-da9842826fc0",
   "metadata": {},
   "source": [
    "### Transfer Learning of VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9596a5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load pre-trained VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model and add pre-trained layers\n",
    "model_VGG16_transfer = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_VGG16_transfer.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model_VGG16_transfer.summary()\n",
    "\n",
    "# Define a callback to save the best model during training\n",
    "checkpoint = ModelCheckpoint(\"best_model_vgg16_tl.keras\", monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history_VGG16_transfer = model_VGG16_transfer.fit(\n",
    "    train_generator,\n",
    "    epochs=50,  # Adjust the number of epochs based on your needs\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint],\n",
    "    workers=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e554cba0-e9aa-41ac-b8ba-81e2be513698",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = '/Users/linuszarse/Documents/UNI/Master-Uni Potsdam/3. Semester/Machine Learning 2/Klausurprojekt/Best_Models/'\n",
    "history_filename_VGG16tl = 'VGG16_transfer_model_training_history.json'\n",
    "\n",
    "# Speichern der Historie\n",
    "save_history_to_json(history_VGG16_transfer, history_filename_VGG16tl, history_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19abe242-6967-49d6-a78e-d8f73d9c12a9",
   "metadata": {},
   "source": [
    "***\n",
    "### Evalutation Protocoll: K-Fold Cross Validation for VGG16 Transfer Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54955c10-4268-456e-a97d-c68f1b4494b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_performance_VGG16tl_model, avg_perf_VGG16tl_model, fold_histories_VGG16tl_model = k_fold_cross_validation(model_VGG16_transfer, \n",
    "                                                                                                train_generator, \n",
    "                                                                                                n_splits=5, \n",
    "                                                                                                epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd10c64-5a9b-48d0-aa01-29862b109f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays in histories to lists\n",
    "fold_histories_VGG16tl_model = [\n",
    "    {key: np.array(val).tolist() for key, val in fold_history.items()} \n",
    "    for fold_history in fold_histories_VGG16tl_model  # use the correct variable here\n",
    "]\n",
    "\n",
    "results_VGG16tl = {\n",
    "    \"fold_performance\": fold_performance_VGG16tl_model,\n",
    "    \"average_performance\": avg_perf_VGG16tl_model,\n",
    "    \"fold_histories\": fold_histories_VGG16tl_model\n",
    "}\n",
    "\n",
    "# Speichern der Ergebnisse\n",
    "save_results_to_json('fold_performance_VGG16tl_model_cvr.json', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e201081-465a-42ea-b555-48d0a189ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = 'fold_performance_hp_model_cvr.json'\n",
    "#full_path = os.path.join(default_path, file_name)\n",
    "\n",
    "#with open(full_path, 'r') as file:\n",
    "#    results = json.load(file)\n",
    "\n",
    "# Extract the histories\n",
    "#fold_histories = results['fold_histories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e8b68-487e-4e37-b61f-7aa1c431cea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc989212-7c20-482e-9ac1-a00c3e196ea1",
   "metadata": {},
   "source": [
    "### Fine Tuning of VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1a2de3-bad0-485d-9c67-373c1b346442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine Tuning\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import models, layers\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# 1. Laden des vortrainierten VGG16-Modells\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "# 2. Anpassen des Models an meine Daten\n",
    "vgg16_ft = models.Sequential()\n",
    "vgg16_ft.add(base_model)  # Hinzufügen des vortrainierten Modells als Basis\n",
    "vgg16_ft.add(layers.Flatten())\n",
    "\n",
    "# Hinzufügen einiger Fully-Connected Schichten\n",
    "vgg16_ft.add(layers.Dense(256, activation='relu'))\n",
    "vgg16_ft.add(layers.Dropout(0.5))\n",
    "vgg16_ft.add(layers.Dense(num_classes, activation='softmax')) \n",
    "\n",
    "# 3. Einfrieren aller Schichten im Basis-Modell\n",
    "base_model.trainable = False\n",
    "\n",
    "# 4. Kompilieren und Trainieren des Models\n",
    "vgg16_ft.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define a callback to save the best model during training\n",
    "checkpoint = ModelCheckpoint(\"best_model_vgg16_ft.keras\", \n",
    "                             monitor='val_loss', \n",
    "                             save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Trainieren des Modells mit Ihren eigenen `train_generator` und `validation_generator`\n",
    "history_initial = vgg16_ft.fit(train_generator, \n",
    "                               validation_data=validation_generator, \n",
    "                               epochs=10, \n",
    "                               callbacks=[checkpoint])\n",
    "\n",
    "# 5. Fine-Tuning einiger Schichten des Basis-Modells\n",
    "# Auftauen der oberen Schichten des VGG16-Modells\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Kompilieren des Modells mit einer niedrigeren Lernrate\n",
    "vgg16_ft.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Erneutes Trainieren des Modells mit Fine-Tuning\n",
    "history_fine_tuning = vgg16_ft.fit(train_generator, \n",
    "                                   validation_data=validation_generator, \n",
    "                                   epochs=50, \n",
    "                                   callbacks=[checkpoint, early_stopping],\n",
    "                                   workers=6,\n",
    "                                   class_weight=class_weights_dict)\n",
    "\n",
    "# Display the model summary\n",
    "vgg16_ft.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b57fd-aa25-4bbf-9dfd-09a8673e11df",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = '/Users/linuszarse/Documents/UNI/Master-Uni Potsdam/3. Semester/Machine Learning 2/Klausurprojekt/Best_Models/'\n",
    "history_filename_VGG16ft = 'VGG16_finetuning_model_training_history.json'\n",
    "\n",
    "# Speichern der Historie\n",
    "save_history_to_json(history_fine_tuning, history_filename_VGG16ft, history_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520db47a-e44e-4e08-a0c8-74029ee81c94",
   "metadata": {},
   "source": [
    "***\n",
    "### Evalutation Protocoll: K-Fold Cross Validation for VGG16 Fine Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1026d872-0ac6-4d00-9eef-4856db081ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_performance_VGG16ft_model, avg_perf_VGG16ft_model, fold_histories_VGG16ft_model = k_fold_cross_validation(vgg16_ft, \n",
    "                                                                                                train_generator, \n",
    "                                                                                                n_splits=5, \n",
    "                                                                                                epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeedd4b7-aaf4-4300-8e2d-6416ba08f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays in histories to lists\n",
    "fold_histories_VGG16tl_model = [\n",
    "    {key: np.array(val).tolist() for key, val in fold_history.items()} \n",
    "    for fold_history in fold_histories_VGG16ft_model  # use the correct variable here\n",
    "]\n",
    "\n",
    "results_VGG16tl = {\n",
    "    \"fold_performance\": fold_performance_VGG16ft_model,\n",
    "    \"average_performance\": avg_perf_VGG16ft_model,\n",
    "    \"fold_histories\": fold_histories_VGG16ft_model\n",
    "}\n",
    "\n",
    "# Speichern der Ergebnisse\n",
    "save_results_to_json('fold_performance_VGG16finet_model_cvr.json', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a0954-c819-4031-a28c-c736199a419c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdc61593",
   "metadata": {},
   "source": [
    "***\n",
    "## RESNET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb30de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet50 model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model and add pre-trained layers\n",
    "model_resnet = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_resnet.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model_resnet.summary()\n",
    "\n",
    "# Define a callback to save the best model during training\n",
    "checkpoint = ModelCheckpoint(\"best_model_resnet.keras\", monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history_resnet = model_resnet.fit(\n",
    "    train_generator,\n",
    "    epochs=50,  # Adjust the number of epochs based on your needs\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint],\n",
    "    workers=6,\n",
    "    class_weight=class_weights_dict\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8456c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = '/Users/linuszarse/Documents/UNI/Master-Uni Potsdam/3. Semester/Machine Learning 2/Klausurprojekt/Best_Models/'\n",
    "history_filename_resnet = 'VGG16_finetuning_model_training_history.json'\n",
    "\n",
    "# Speichern der Historie\n",
    "save_history_to_json(history_resnet, history_filename_resnet, history_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5195f70-1d86-4331-ba3f-4f962a87aebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c877f5c-76b2-40c4-b642-ac7f656d8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_performance_resnet_model, avg_perf_resnet_model, fold_histories_resnet_model = k_fold_cross_validation(model_resnet, \n",
    "                                                                                                train_generator, \n",
    "                                                                                                n_splits=5, \n",
    "                                                                                                epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e3817-e744-4eb1-b97a-f97fa688269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays in histories to lists\n",
    "fold_histories_VGG16tl_model = [\n",
    "    {key: np.array(val).tolist() for key, val in fold_history.items()} \n",
    "    for fold_history in fold_histories_resnet_model  # use the correct variable here\n",
    "]\n",
    "\n",
    "results_VGG16tl = {\n",
    "    \"fold_performance\": fold_performance_resnet_model,\n",
    "    \"average_performance\": avg_perf_resnet_model,\n",
    "    \"fold_histories\": fold_histories_resnet_model\n",
    "}\n",
    "\n",
    "# Speichern der Ergebnisse\n",
    "save_results_to_json('fold_performance_resnet_model_cvr.json', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a5869-5771-43f7-ba4d-2d961de23383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2496462b",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Hypothesis test of model comparition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c954116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary mapping models to their names\n",
    "\n",
    "models = [model_cnn1, model_cnn2, model_cnn3, model_VGG16, model_resnet]\n",
    "\n",
    "model_names = {\n",
    "    model_cnn1: \"model_cnn1\",\n",
    "    model_cnn2: \"model_cnn2\",\n",
    "    model_cnn3: \"model_cnn3\",\n",
    "    model_VGG16: \"model_VGG16\",\n",
    "    model_resnet: \"model_resnet\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_models(model_A, model_B, validation_generator, model_names, alpha=0.05):\n",
    "    predictions_A = []\n",
    "    predictions_B = []\n",
    "    true_labels = []\n",
    "\n",
    "    for i in range(len(validation_generator)):\n",
    "        X, y = validation_generator[i]\n",
    "        true_labels.extend(y)\n",
    "\n",
    "        preds_A = model_A.predict(X, verbose=0)\n",
    "        preds_B = model_B.predict(X, verbose=0)\n",
    "\n",
    "        predictions_A.extend(preds_A)\n",
    "        predictions_B.extend(preds_B)\n",
    "\n",
    "    predictions_A = np.array(predictions_A)\n",
    "    predictions_B = np.array(predictions_B)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    predicted_labels_A = np.argmax(predictions_A, axis=1)\n",
    "    predicted_labels_B = np.argmax(predictions_B, axis=1)\n",
    "    true_labels = np.argmax(true_labels, axis=1)\n",
    "\n",
    "    accuracy_A = accuracy_score(true_labels, predicted_labels_A)\n",
    "    accuracy_B = accuracy_score(true_labels, predicted_labels_B)\n",
    "\n",
    "    t_statistic, p_value = ttest_rel(predicted_labels_A, predicted_labels_B)\n",
    "\n",
    "    model_A_name = model_names.get(model_A, 'Unknown Model A')\n",
    "    model_B_name = model_names.get(model_B, 'Unknown Model B')\n",
    "    \n",
    "    # Print a blank line for separation and then the comparison header\n",
    "    print(\"\\nComparison between\", model_A_name, \"and\", model_B_name)\n",
    "\n",
    "    print(f\"Accuracy {model_A_name}: {accuracy_A:.4f}\")\n",
    "    print(f\"Accuracy {model_B_name}: {accuracy_B:.4f}\")\n",
    "    print(f\"T-Statistic: {t_statistic:.4f}\")\n",
    "    print(f\"P-Value: {p_value:.4f}\")\n",
    "\n",
    "    if p_value < alpha:\n",
    "        print(\"Reject the null hypothesis. There is a significant difference.\")\n",
    "        if accuracy_A > accuracy_B:\n",
    "            print(f\"{model_A_name} is statistically significantly better than {model_B_name}.\")\n",
    "        else:\n",
    "            print(f\"{model_B_name} is statistically significantly better than {model_A_name}.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis. No significant difference.\")\n",
    "        if accuracy_A == accuracy_B:\n",
    "            print(f\"Both {model_A_name} and {model_B_name} have similar performance.\")\n",
    "        elif accuracy_A > accuracy_B:\n",
    "            print(f\"{model_A_name} is better, but not significantly.\")\n",
    "        else:\n",
    "            print(f\"{model_B_name} is better, but not significantly.\")\n",
    "\n",
    "    return accuracy_A, accuracy_B, p_value, \"A\" if accuracy_A > accuracy_B else \"B\" if accuracy_B > accuracy_A else \"None\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c10aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(model_cnn1, model_cnn2, test_generator, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f15dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_cnn1, model_cnn2, model_cnn3, model_VGG16, model_resnet]\n",
    "model_scores = [0] * len(models)\n",
    "\n",
    "for i in range(len(models)):\n",
    "    for j in range(i + 1, len(models)):\n",
    "        accuracy_A, accuracy_B, p_value, result = compare_models(models[i], models[j], validation_generator, model_names)\n",
    "        \n",
    "        # Update scores based on comparison results\n",
    "        if result == \"A\":\n",
    "            model_scores[i] += 1\n",
    "        elif result == \"B\":\n",
    "            model_scores[j] += 1\n",
    "        # No update if result is \"None\" as there's no significant difference\n",
    "\n",
    "# Rank models based on scores\n",
    "ranked_models = sorted(range(len(models)), key=lambda x: model_scores[x], reverse=True)\n",
    "\n",
    "# Print out the ranking\n",
    "for rank, model_index in enumerate(ranked_models):\n",
    "    model_name = model_names.get(models[model_index], f\"Model {model_index + 1}\")\n",
    "    print(f\"Rank {rank + 1}: {model_name} with {model_scores[model_index]} wins\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc171b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31007e3e",
   "metadata": {},
   "source": [
    "# Points to think about:\n",
    "* generalisation of the model -> do not overfit\n",
    "* Hypothesis testing \n",
    "* Evaluation Protocolls \n",
    "    * K-Fold Cross Validation\n",
    "    * Nested Cross Validation \n",
    "* what is the best architecture for CNN???\n",
    "* Hyperparameter TUNING\n",
    "* Why Clustering does not make sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689c4e4",
   "metadata": {},
   "source": [
    "## Websites where i have to look in \n",
    "\n",
    "* https://www.jeremyjordan.me/convnet-architectures/\n",
    "* https://github.com/pedropro/TACO\n",
    "* https://github.com/AgaMiko/waste-datasets-review/tree/main/img\n",
    "* https://github.com/settings/copilot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c052b",
   "metadata": {},
   "source": [
    "Certainly! In addition to hypothesis testing, there are several other aspects you might want to check to ensure a comprehensive evaluation of your classification model. Here are some key considerations:\n",
    "\n",
    "1. Confusion Matrix and Classification Report:\n",
    "    * Analyze the confusion matrix to understand how well your model is performing for each class. It provides insights into true positives, true negatives, false positives, and false negatives.\n",
    "    * Use the classification report to obtain precision, recall, and F1-score for each class.\n",
    "2. ROC Curve and AUC:\n",
    "    * If your problem is binary classification, consider plotting the ROC curve and calculating the Area Under the Curve (AUC) to assess the model's ability to discriminate between classes.\n",
    "3. Cross-Validation:\n",
    "    * Implement k-fold cross-validation to evaluate the model's performance on multiple train-test splits. This helps assess the model's generalization across different subsets of the data.\n",
    "4. Learning Curves:\n",
    "    * Plot learning curves to visualize the model's training and validation performance over epochs. This helps identify overfitting or underfitting issues.\n",
    "5. Feature Importance:\n",
    "    * If applicable, investigate feature importance. In image classification, this might not be as straightforward, but if you have extracted features from the images or are using a graph-based model, consider analyzing feature importance.\n",
    "6. Model Interpretability:\n",
    "    * Utilize techniques for model interpretability, such as SHAP (SHapley Additive exPlanations) values or LIME (Local Interpretable Model-agnostic Explanations), to understand the model's decision-making process.\n",
    "7. Hyperparameter Tuning:\n",
    "    * Experiment with different hyperparameter settings to optimize model performance. This could include learning rate, batch size, number of layers, etc.\n",
    "8. Handling Class Imbalance:\n",
    "    * If your dataset has imbalanced classes, explore techniques such as oversampling, undersampling, or using class weights to address the imbalance.\n",
    "9. Error Analysis:\n",
    "    * Analyze specific examples where the model fails to make accurate predictions. Understanding the types of errors can provide insights into potential improvements.\n",
    "10. Deployment Considerations:\n",
    "    * If you plan to deploy the model, consider additional factors such as model size, computational efficiency, and inference speed.\n",
    "    * Remember that the evaluation process is an iterative one, and it's essential to tailor it to the specifics of your project and dataset. Adjust your evaluation strategy based on the insights gained from each step to iteratively improve your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b475b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
